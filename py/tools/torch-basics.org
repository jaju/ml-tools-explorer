#+property: header-args:python+ :exports both :results output :session *python* :tangle torch_basics.py
#+title: Torch Basics

Let us explore some basic building blocks and what they accomplish, to get a better sense of how things add up. So, we are not looking to *apply* torch as much as explore what it offers as part of the library.

#+begin_src python
import torch
import torch.nn as nn
import torch.optim as optim
#+end_src

#+RESULTS:

* Tensors

Let's create a 1-D vector of 10 random floats. Notice the output of the ~type~ function on the vector. ~torch.Tensor~ is more than just the tensor it purports to be.

#+begin_src python :exports both
rnd_vec = torch.randn(10)
print(type(rnd_vec))
print(rnd_vec)
#+end_src

#+RESULTS:
: <class 'torch.Tensor'>
: tensor([-0.0183, -0.3427, -0.1544, -0.0145, -1.2310,  0.4460,  0.5361,  0.4710,
:         -0.5114,  0.6465])



** Views on Tensors

We can project views from this tensor rnd_vec. Notice the order in which values are printed for each view - they are the same, read linearly. But the grouping happens as per the suggested view shape.

#+begin_src python :exports both
view_vec = rnd_vec.view(-1)
print(view_vec)

# 5 groups of 2 each
view_matrix = rnd_vec.view(5, 2)
print(view_matrix)

# 2 groups of 5 groups of 1 each
view_3d = rnd_vec.view(2, 5, 1)
print(view_3d)
#+end_src

#+RESULTS:
#+begin_example
tensor([-0.0183, -0.3427, -0.1544, -0.0145, -1.2310,  0.4460,  0.5361,  0.4710,
        -0.5114,  0.6465])
tensor([[-0.0183, -0.3427],
        [-0.1544, -0.0145],
        [-1.2310,  0.4460],
        [ 0.5361,  0.4710],
        [-0.5114,  0.6465]])
tensor([[[-0.0183],
         [-0.3427],
         [-0.1544],
         [-0.0145],
         [-1.2310]],

        [[ 0.4460],
         [ 0.5361],
         [ 0.4710],
         [-0.5114],
         [ 0.6465]]])
#+end_example

It'd be misleading to consider it to be just a tensor. It's made to carry additional responsibilities and data to record information like the change deltas and the gradient functions that are key pieces of information used during the learning phase.

Let us next execute a simple linear regression example. ~nn.Module~ is a basic building block for creating a chain of operations on an input tensor, leading to an output tensor. The following example is a chain of *one*. We use the ~nn.Linear~ concrete class within our custom implementation.

#+begin_src python :exports both
class LinearRegressor(nn.Module):
    def __init__(self):
        super(LinearRegressor, self).__init__()
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        return self.linear(x)

model = LinearRegressor()

for name, param in model.named_parameters():
    if param.requires_grad:
        print(name, param)
#+end_src

#+RESULTS:
: linear.weight Parameter containing:
: tensor([[-0.5207]], requires_grad=True)
: linear.bias Parameter containing:
: tensor([-0.7738], requires_grad=True)

The output shows ~weight~ and ~bias~ in the module initialized from ~nn.Linear~. You can draw an analogy to $wx + b = y$ with ~w~ as the weight and ~b~ as the bias.

As we introduce this "model" to our training dataset - created as a simple $y = 3x +2$, we need as mechanism to help it understand how far away it is from the expected $y$ values - the _criterion_ - and how to fix the weight to reduce this error - the _optimizer_. Pytorch provides various such measurement and update classes.

#+begin_src python :exports both
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

x_train = torch.randn(10, 1)
y_train = 3 * x_train + 2

model.train()

for epoch in range(1000):
    optimizer.zero_grad()
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

model.eval()

x_test = torch.tensor([5.0])
y_test = model(x_test)

print(y_test.item())
#+end_src

#+RESULTS:
: 16.999998092651367

We started with random weights, which need to be adjusted. The amount and direction of adjustment is derived using calculus - what change in the weights and biases can lead to the outcome $y$ moving in the right direction? Pytorch Tensor-s can compute and store these gradients for $w$ and $b$ (also called, the parameters), and make them available when you are computing the adjustments. The actual update value is scaled by the learning rate _lr_.

Our training data is an ideal training data - the dependent $y$ is related to $x$ as $3x + 2$. The model is initialized with random values for the weight and the bias. In a loop, then we calculate the delta of the prediction from the actual, and use that to propagate update backwards. Pytorch tensors have the infrastructure to remember the changes and supply them to the optimizer so that it can calculate the change needed to move the parameters of the ~layer~ in the right direction, so as to reduce the ~loss~ in the next step - or epoch.

#+begin_src python :results both
x2_test = torch.randn(5, 1)
y2_test = model(x2_test)

# The values printed should be vanishingly small
print((x2_test * 3 + 2) - y2_test)
#+end_src

#+RESULTS:
: tensor([[ 1.1921e-07],
:         [ 2.3842e-07],
:         [-7.1526e-07],
:         [ 0.0000e+00],
:         [-7.1526e-07]], grad_fn=<SubBackward0>)

Experimenting with different values for the number of epochs and the learning rate is useful - to gain some good understanding of how they affect convergence for different kinds of data, networks, and choices of optimizers and loss functions.

#+begin_src python :exports both
def create_model(x_train, y_train, epochs=1000, lr=0.1):
    model = LinearRegressor()
    loss_criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(x_train)
        loss = loss_criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
    model.eval()
    return model
#+end_src

#+RESULTS:

Let us experiment with varied epochs and learning rates
#+begin_src python :exports both
y2_ideal = x2_test * 3 + 2
model1 = create_model(x_train, y_train, 1000, 1)
print(model1(x2_test) - y2_ideal)

model2 = create_model(x_train, y_train, 1000, 0.01)
print(model2(x2_test) - y2_ideal)

model3 = create_model(x_train, y_train, 1000, 0.1)
print(model3(x2_test) - y2_ideal)
#+end_src

#+RESULTS:
#+begin_example
tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan]], grad_fn=<SubBackward0>)
tensor([[-9.5367e-07],
        [-4.7684e-06],
        [ 7.8678e-06],
        [ 7.1526e-07],
        [ 6.9141e-06]], grad_fn=<SubBackward0>)
tensor([[-1.1921e-07],
        [-2.3842e-07],
        [ 7.1526e-07],
        [ 0.0000e+00],
        [ 7.1526e-07]], grad_fn=<SubBackward0>)
#+end_example

"A quick learner is just a yes man. A slow learner is better."

The outcomes based on the choices are not random, but might appear so to the untrained mind. But that's the spectrum of differences between the experts and the novice, and getting great results versus not from the same datasets ðŸ˜‡.

* Broadcasting

It's complicated. But it exists. So, when you need to use it, study it. You will sure forget it, until you need it again. When you study it again. And then just move on, until the next time.

#+begin_src python :exports both
x = torch.ones(())
y = torch.ones(3, 1)
z = torch.ones(1, 3)
a = torch.ones(2, 1, 1)

print("shapes")
print(f"x: {x.shape}")
print(f"y: {y.shape}")
print(f"z: {z.shape}")
print(f"a: {a.shape}")
print(f"x * y        :", (x * y).shape)
print(f"    y * z    :", (y * z).shape)
print(f"    y * z * a:", (y * z * a).shape)
#+end_src

#+RESULTS:
: shapes
: x: torch.Size([])
: y: torch.Size([3, 1])
: z: torch.Size([1, 3])
: a: torch.Size([2, 1, 1])
: x * y        : torch.Size([3, 1])
:     y * z    : torch.Size([3, 3])
:     y * z * a: torch.Size([2, 3, 3])

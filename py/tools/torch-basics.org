#+property: header-args:python+ :exports both :results output :session *python* :tangle torch_basics.py
#+title: Torch Basics

Let us explore some basic building blocks and what they accomplish, to get a better sense of how things add up. So, we are not looking to *apply* torch as much as explore what it offers as part of the library.

#+begin_src python
import torch
import torch.nn as nn
import torch.optim as optim
#+end_src

#+RESULTS:

Let's create a 1-D vector of 10 random floats. Notice the output of the ~type~ function on the vector. ~torch.Tensor~ is more than just the tensor it purports to be.

#+begin_src python :exports both
rnd_vec = torch.randn(10)
print(type(rnd_vec))
print(rnd_vec)
#+end_src

#+RESULTS:
: <class 'torch.Tensor'>
: tensor([-0.2842,  0.3082, -0.7347, -0.5907,  0.0441, -0.4838, -0.2542,  1.3246,
:         -1.0247,  0.6760])


It'd be misleading to consider it to be just a tensor. Took me a while to realise. It's made to carry additional responsibilities and data to record information like the change deltas and the gradient functions that are key pieces of information used during the learning phase.

We can project views from this tensor rnd_vec.
#+begin_src python
view_vec = rnd_vec.view(-1)
print(view_vec)

view_matrix = rnd_vec.view(5, 2)
print(view_matrix)

view_3d = rnd_vec.view(2, 5, 1)
print(view_3d)
#+end_src

#+RESULTS:
#+begin_example
tensor([-0.2842,  0.3082, -0.7347, -0.5907,  0.0441, -0.4838, -0.2542,  1.3246,
        -1.0247,  0.6760])
tensor([[-0.2842,  0.3082],
        [-0.7347, -0.5907],
        [ 0.0441, -0.4838],
        [-0.2542,  1.3246],
        [-1.0247,  0.6760]])
tensor([[[-0.2842],
         [ 0.3082],
         [-0.7347],
         [-0.5907],
         [ 0.0441]],

        [[-0.4838],
         [-0.2542],
         [ 1.3246],
         [-1.0247],
         [ 0.6760]]])
#+end_example


Let us next execute a simple linear regression example. ~nn.Module~ is a basic building block for creating a chain of operations on an input tensor, leading to an output tensor. The following example is a chain of *one*. We use the ~nn.Linear~ concrete class within our custom implementation.

#+begin_src python
class LinearRegressor(nn.Module):
    def __init__(self):
        super(LinearRegressor, self).__init__()
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        return self.linear(x)

model = LinearRegressor()

for name, param in model.named_parameters():
    if param.requires_grad:
        print(name, param)
#+end_src

#+RESULTS:
: linear.weight Parameter containing:
: tensor([[0.2917]], requires_grad=True)
: linear.bias Parameter containing:
: tensor([-0.3132], requires_grad=True)

The output shows ~weight~ and ~bias~ in the module initialized from ~nn.Linear~. Analogy to $ax + b = y$ is straightforwad - ~a~ is the weight and ~b~ is the bias.

#+begin_src python
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

x_train = torch.randn(10, 1)
y_train = 3 * x_train + 2

model.train()

for epoch in range(1000):
    optimizer.zero_grad()
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

model.eval()

x_test = torch.tensor([5.0])
y_test = model(x_test)

print(y_test.item())
#+end_src

#+RESULTS:
: 16.99999237060547

Our training data is an ideal training data - the dependent $y$ is related to $x$ as $3x + 2$. The model is initialized with random values for the weight and the bias. In a loop, then we calculate the delta of the prediction from the actual, and use that to propagate update backwards. Pytorch tensors have the infrastructure to remember the changes and supply them to the optimizer so that it can calculate the change needed to move the parameters of the ~layer~ in the right direction, so as to reduce the ~loss~ in the next step - or epoch.

Experimenting with different values for the number of epochs and the learning rate is useful - to gain some good understanding of how they affect convergence for different kinds of data, networks, and choices of optimizers and loss functions.
